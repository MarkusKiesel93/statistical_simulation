---
title: "Statistical Simulation and Computerintensive Methods"
subtitle: "Exercise 5: Sampling Intervals for Models"
author: "Markus Kiesel | 1228952"
date: "11.11.2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
library(knitr)
library(boot)
```

# Task 1

https://www.youtube.com/watch?v=Zet-qmEEfCU

https://www.tau.ac.il/~saharon/StatisticsSeminar_files/Hypothesis.pdf

https://dataz4s.com/statistics/two-sample-bootstrap-hypothesis-testing/

https://en.wikipedia.org/wiki/Student%27s_t-test


https://nblok306.ftn.uns.ac.rs/~zoran/A/knjige/Chihara2011.pdf

http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/501_02_Efron_Introduction-to-the-Bootstrap.pdf

Consider a two sample problem and the hypothesis 𝐻0:𝜇1=𝜇2 vs 𝐻1:𝜇1≠𝜇2 , where 𝜇1 and 𝜇2 are the corresponding sample locations. The two samples are:

```{r}
x1 <- c(-0.673, -0.584, 0.572, -0.341, -0.218, 0.603, -0.415, -0.013, 0.763, 0.804, 0.054, 1.746, -0.472, 1.638, -0.578, 0.947, -0.329, -0.188, 0.794, 0.894, -1.227, 1.059)

x2 <- c(0.913, -0.639, 2.99, -5.004, 3.118, 0.1, 1.128, 0.579, 0.32, -0.488, -0.994, -0.212, 0.413, 1.401, 0.007, 0.568, -0.005, 0.696)
```

```{r}
length(x1)
length(x2)
```
The key in bootstrapping hypothesis tests is that the sampling
should reflect the distribution under H0 . Even if the data at hand
was actually not following it!


sample size < 30 we want to use the t-distribution


1.1 Plot the data in a way which visualizes the comparison of means appropriately.

todo: why did I plot density ? 

```{r, fig.height=4, fig.width=10}
par(mfrow = c(1,2))
m <- 10000
# x1.Bmeans <- replicate(m, mean(sample(x1, replace=TRUE)))
# x2.Bmeans <- replicate(m, mean(sample(x2, replace=TRUE)))
plot(density(x1), 
     type = "l", 
     col = "red",
     main = "Distribution of bootstrap means")
abline(v=mean(x1), col="red")
lines(density(x2), col = "blue")
abline(v=mean(x2), col="blue")
legend("topright",
       legend = c("x1 Bootstrap means", "x2 Bootstrap means"),
       pch = c(15,15),
       col = c("red", "blue"))
boxplot(x1, x2)
```

1.2 Consider different sampling schemes, such as 
Sampling with replacement from each group
Centering both samples and then resample from the combined samples 𝑛1 and 𝑛2 times.
Argue for choice what is more natural and which advantages or disadvantages may apply.

this is more natural if we consider two independent samples but we can encounter problems because the saple size is not the same for both saples and the variance differs for both samples

The second choice is more natural if we want to do test on dependent samples

https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test

```{r}

```


Next we try centering both samples and resampling from the combined samples.




1.3 Bootstrap using both strategies mentioned above using the t-test statistic. Calculate the bootstrap p-value based on 10000 bootstrap samples and 0.95 as well as 0.99 confidence intervals. Make your decision at the significance level 0.05 or 0.01, respectively.

https://www.youtube.com/watch?v=Kho4VuKmQdE

First we follow the approach for sampling with replacement from each group.

```{r}
# number of bootstrap samples
m <- 10000
# size of samples
n1 <- length(x1)
n2 <- length(x2)
# standard diviation of original samples
sd1 <- sd(x1)
sd2 <- sd(x2)
# t statistic to compare 2 groups
Tstatistic <- function(x, y) {
        (mean(x) - mean(y)) / (sqrt(sd(x) * 1/n1 + sd(y) * 1/n2))
}
# t statistic for original samples
TX <- Tstatistic(x1, x2)
# create t statistics for all m bootstrap samples
Tstarnonparam <- replicate(m, Tstatistic(
        x = sample(x1, replace = TRUE),
        y = sample(x2, replace = TRUE)
))
# caluclate bootstrap p-value
(sum(abs(Tstarnonparam) > abs(TX)) + 1) / (m + 1)
# 0.95 confidence interval
quantile(Tstarnonparam, c(0.025, 0.975))
# 0.99 confidence interval
quantile(Tstarnonparam, c(0.005, 0.995))
```


```{r}
# number of bootstrap samples
m <- 10000
# size of samples
n1 <- length(x1)
n2 <- length(x2)
# centering data
x <- c(x1, x2)
xbar <- mean(x)
# center data so they share a common mean
x1.cent <- x1 - mean(x1) + xbar
x2.cent <- x2 - mean(x2) + xbar
x.cent <- c(x1.cent, x2.cent)
mu0 <- mean(x.cent)
# t statistic
Tstatistic <- function(x, mu0) {
        (mean(x) - mu0) / sd(x)
}

TX <- Tstatistic(x, mu0=mu0)

Tstarnonparam <- replicate(m, Tstatistic(
        x = sample(x1, replace = TRUE),
        mu0 = xbar
))

(sum(abs(Tstarnonparam) > abs(TX)) + 1) / (m + 1)

```

1.4 What would be a permutation version of the test? Implement the corresponding permutation test and obtain p-value and confidence intervals as in 3. to get a corresponding test decision at the same significance levels.

```{r}

```

1.5 The Wilxocon rank sum test statistic is the sum of ranks of the observations of sample 1 computed in the combined sample. Use bootstrapping with both strategies mentioned above and obtain p-value and confidence intervals as in 3. to get a corresponding test decision at the same significance levels.

```{r}

```

1.6 Compare your results to the results using t.test and wilcox.test.

```{r}
t.test(x1, x2)

wilcox.test(x1, x2)
```

# Task 2

Consider the model 𝑦=3+2⋅𝑥1+𝑥2+𝜀 where 𝑥1 comes from a normal distribution with mean 2 and variance 3, 𝑥2 comes from a uniform distribution between 2 and 4 and 𝜀 from a student's t distribution with 5 degrees of freedom . In addition, there is a predictor 𝑥3 coming from a uniform distribution between -2 and 2.

```{r}

```

1.1 Create a sample of size 200 from the model above and for the independent predictor 𝑥.

```{r}
n <- 200
x1 <- rnorm(n, 2, sqrt(3))
x2 <- runif(n, 2, 4)
eps <- rt(n, 5)
y = 3 + 2 * x1 + x2 + eps

x3 <- runif(n, -2, 2)
```

1.2 Do residual bootstrap for linear regression and fit the model 𝑦∼𝑥1+𝑥2+𝑥3 . Get the percentile CI for the coefficients. Can you exclude 𝑥3 ?

```{r}

```

1.3 Do pairs bootstrap for linear regression and fit the model 𝑦∼𝑥1+𝑥2+𝑥3 . Get the percentile CI for the coefficients. Can you exclude 𝑥3 ?

```{r}

```

1.4 Compare the two approaches in 2. and 3. and explain the differences in the sampling approach and how this (might) affect(s) the results.

```{r}

```

# Task 3

Summarise the bootstrapping methodology, its advantages and disadvantages based on your exercises for constructing parametric and non-paramteric confidence bounds for estimators, test statistics or model estimates.

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```


</div></pre>